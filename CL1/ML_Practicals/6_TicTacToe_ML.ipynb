{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a128d8-d6e9-4164-ae73-ba6c7974e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300b3504-f505-4270-930f-1ea6d5b898d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tic-Tac-Toe environment setup\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the board to an empty state\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.done = False\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Flatten the board as the state representation\n",
    "        return tuple(self.board.flatten())\n",
    "    \n",
    "    def available_actions(self):\n",
    "        # Return a list of available actions (empty cells)\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "    \n",
    "    def step(self, action, player):\n",
    "        # Perform the action on the board\n",
    "        if self.board[action] == 0:\n",
    "            self.board[action] = player\n",
    "            reward, self.done = self.check_winner(player)\n",
    "            return self.get_state(), reward, self.done\n",
    "        else:\n",
    "            return self.get_state(), -1, self.done  # Illegal move\n",
    "    \n",
    "    def check_winner(self, player):\n",
    "        # Check if the current player has won\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
    "                return 1, True  # Reward of 1 for winning\n",
    "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "            return 1, True\n",
    "        if not any(0 in row for row in self.board):\n",
    "            return 0.5, True  # Reward of 0.5 for draw\n",
    "        return 0, False  # No reward if the game continues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361fb4b3-7efe-4ba8-91f7-7e7329c079be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game display function for visual debugging\n",
    "def display_board(board):\n",
    "    board_symbols = {0: '.', 1: 'X', 2: 'O'}\n",
    "    for row in board:\n",
    "        print(\" | \".join([board_symbols[cell] for cell in row]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17de49f3-ce4e-467e-a542-16c7ed58b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "    \n",
    "    def choose_action(self, state, available_actions):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.choice(available_actions)\n",
    "        else:\n",
    "            q_values = {action: self.q_table[(state, action)] for action in available_actions}\n",
    "            return max(q_values, key=q_values.get)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done, available_actions):\n",
    "        old_q_value = self.q_table[(state, action)]\n",
    "        next_max = max([self.q_table[(next_state, a)] for a in available_actions], default=0)\n",
    "        new_q_value = (1 - self.learning_rate) * old_q_value + self.learning_rate * (reward + self.discount_factor * next_max * (1 - done))\n",
    "        self.q_table[(state, action)] = new_q_value\n",
    "        if done:\n",
    "            self.exploration_rate *= self.exploration_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c52e48-83ca-4d63-9653-bd624ea67b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "env = TicTacToe()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        available_actions = env.available_actions()\n",
    "        action = agent.choose_action(state, available_actions)\n",
    "        next_state, reward, done = env.step(action, 1)\n",
    "        agent.update_q_value(state, action, reward, next_state, done, available_actions)\n",
    "        state = next_state\n",
    "\n",
    "        if not done:\n",
    "            # Simulate random opponent's move\n",
    "            opp_action = random.choice(env.available_actions())\n",
    "            next_state, reward, done = env.step(opp_action, 2)\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c415d0cb-ddc1-428e-9c37-c040a888419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tic-Tac-Toe Game!\n",
      ". | . | .\n",
      ". | . | .\n",
      ". | . | .\n",
      "\n",
      "X | . | .\n",
      ". | . | .\n",
      ". | . | .\n",
      "\n",
      "X | . | .\n",
      ". | . | .\n",
      ". | . | O\n",
      "\n",
      "X | X | .\n",
      ". | . | .\n",
      ". | . | O\n",
      "\n",
      "X | X | .\n",
      ". | O | .\n",
      ". | . | O\n",
      "\n",
      "X | X | X\n",
      ". | O | .\n",
      ". | . | O\n",
      "\n",
      "Agent wins!\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Starting Tic-Tac-Toe Game!\")\n",
    "display_board(env.board)\n",
    "\n",
    "while not done:\n",
    "    # Agent's move\n",
    "    available_actions = env.available_actions()\n",
    "    action = agent.choose_action(state, available_actions)\n",
    "    next_state, reward, done = env.step(action, 1)\n",
    "    state = next_state\n",
    "    display_board(env.board)\n",
    "\n",
    "    if done:\n",
    "        if reward == 1:\n",
    "            print(\"Agent wins!\")\n",
    "        elif reward == 0.5:\n",
    "            print(\"It's a draw!\")\n",
    "        break\n",
    "\n",
    "    # Opponent's move\n",
    "    if not done:\n",
    "        opp_action = random.choice(env.available_actions())\n",
    "        next_state, reward, done = env.step(opp_action, 2)\n",
    "        state = next_state\n",
    "        display_board(env.board)\n",
    "\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                print(\"Opponent wins!\")\n",
    "            elif reward == 0.5:\n",
    "                print(\"It's a draw!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ab822-7210-41db-b095-27f5bdc17d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
